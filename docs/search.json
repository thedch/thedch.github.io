[
  {
    "objectID": "posts/post-with-code/post.html",
    "href": "posts/post-with-code/post.html",
    "title": "Jupyter Testing",
    "section": "",
    "text": "!pwd\n\n/Users/daniel.hunter/code/myblog/posts/post-with-code\n\n\n\nimg = Image.open(Path.home() / 'Downloads' / 'AvatarAI.me' / '61.jpg')\n\n\nimg\n\n\n\n\n\nimport numpy as np\nimport torch\nfrom torchvision import transforms\n\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\n\npreprocess = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n\n\ndef deprocess(image_np):\n    image_np = image_np.squeeze().transpose(1, 2, 0)\n    image_np = image_np * std.reshape((1, 1, 3)) + mean.reshape((1, 1, 3))\n    image_np = np.clip(image_np, 0.0, 255.0)\n    return image_np\n\n\ndef clip(image_tensor):\n    for c in range(3):\n        m, s = mean[c], std[c]\n        image_tensor[0, c] = torch.clamp(image_tensor[0, c], -m / s, (1 - m) / s)\n    return image_tensor\n\n\nfrom torch.autograd import Variable\nfrom tqdm.auto import tqdm\nimport scipy.ndimage as nd\n\n\ndef dream(image, model, iterations, lr):\n    \"\"\" Updates the image to maximize outputs for n iterations \"\"\"\n    Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n    image = Variable(Tensor(image), requires_grad=True)\n    for i in range(iterations):\n        model.zero_grad()\n        out = model(image)\n        # label = [00000,1,00000]\n        label = torch.zeros(1, dtype=torch.long)\n        label[0] = 779\n        loss = nn.CrossEntropyLoss()(out, label)\n        # import pdb ; pdb.set_trace()\n        # loss = out.norm()\n        loss.backward()\n        avg_grad = np.abs(image.grad.data.cpu().numpy()).mean()\n        norm_lr = lr / avg_grad\n        image.data += norm_lr * image.grad.data\n        image.data = clip(image.data)\n        image.grad.data.zero_()\n    return image.cpu().data.numpy()\n\n\ndef deep_dream(image, model, iterations, lr, octave_scale, num_octaves):\n    \"\"\" Main deep dream method \"\"\"\n    image = preprocess(image).unsqueeze(0).cpu().data.numpy()\n\n    # Extract image representations for each octave\n    octaves = [image]\n    for _ in range(num_octaves - 1):\n        octaves.append(nd.zoom(octaves[-1], (1, 1, 1 / octave_scale, 1 / octave_scale), order=1))\n\n    detail = np.zeros_like(octaves[-1])\n    for octave, octave_base in enumerate(tqdm(octaves[::-1], desc=\"Dreaming\")):\n        if octave > 0:\n            # Upsample detail to new octave dimension\n            detail = nd.zoom(detail, np.array(octave_base.shape) / np.array(detail.shape), order=1)\n        # Add deep dream detail from previous octave to new base\n        input_image = octave_base + detail\n        # Get new deep dream image\n        dreamed_image = dream(input_image, model, iterations, lr)\n        # Extract deep dream details\n        detail = dreamed_image - octave_base\n\n    return deprocess(dreamed_image)\n\n\nimport torch\nfrom torch import nn\nfrom torchvision import models\n\n# Define the model\nnetwork = models.vgg19(weights=models.VGG19_Weights.DEFAULT)\n# layers = list(network.features.children())\nmodel = network # nn.Sequential(*layers[: (27 + 1)])\nif torch.cuda.is_available():\n    model = model.cuda()\n# print(network)\n\n# Extract deep dream image\ndreamed_image = deep_dream(\n    img,\n    model,\n    iterations=30,\n    lr=0.03,\n    octave_scale=1.4,\n    num_octaves=5,\n)\n\n\n\n\n\nviz(dreamed_image)\n\n\n\n\n\nnp.array(img).shape\n\n(512, 512, 3)\n\n\n\nnd.zoom(np.array(img), (1 / 1.4, 1 / 1.4, 1), order=1).shape\n\n(366, 366, 3)\n\n\n\nlen(dreamed_image)\n\n10\n\n\n\nfrom more_itertools import zip_equal\nimport matplotlib.pyplot as plt\n\ndef prep_img(arr):\n    if isinstance(arr, Image.Image):\n        arr = np.array(arr)\n    arr = arr.squeeze()\n    if arr.shape[0] == 3:\n        arr = arr.transpose((1,2,0))\n    _H, _W, C = arr.shape\n    assert C == 3, arr.shape\n    if arr.dtype in [np.float32, np.float64]:\n        arr = np.clip(a=arr, a_min=0, a_max=1)\n    elif arr.dtype in [np.uint8]:\n        arr = np.clip(a=arr, a_min=0, a_max=255)\n    else:\n        raise ValueError(arr.dtype)\n    return arr\n\ndef viz(*arrs, ncols=1):\n    fig, axs = plt.subplots(nrows=len(arrs), ncols=ncols, figsize=(20,20), squeeze=False)\n    for arr, ax in zip_equal(arrs, axs.ravel()):\n        # import pdb ; pdb.set_trace()\n        ax.imshow(prep_img(arr))\n\n\nviz(\n    # dreamed_image[0],\n    # dreamed_image[5],\n    # dreamed_image[9],\n    nd.zoom(np.array(img), (1 / 3, 1 / 3, 1), order=1),\n    img,\n)"
  },
  {
    "objectID": "posts/deep-dream/post.html",
    "href": "posts/deep-dream/post.html",
    "title": "Getting Quarto to execute code blocks",
    "section": "",
    "text": "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus."
  },
  {
    "objectID": "posts/deep-dream/post.html#merriweather",
    "href": "posts/deep-dream/post.html#merriweather",
    "title": "Getting Quarto to execute code blocks",
    "section": "Merriweather",
    "text": "Merriweather\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Nam suscipit est nec dui eleifend, at dictum elit ullamcorper. Aliquam feugiat dictum bibendum. Praesent fermentum laoreet quam, cursus volutpat odio dapibus in. Fusce luctus porttitor vehicula. Donec ac tortor nisi. Donec at lectus tortor. Morbi tempor, nibh non euismod viverra, metus arcu aliquet elit, sed fringilla urna leo vel purus."
  },
  {
    "objectID": "posts/deep-dream/post.html#code",
    "href": "posts/deep-dream/post.html#code",
    "title": "Getting Quarto to execute code blocks",
    "section": "3 - Code",
    "text": "3 - Code\nThis is inline code plus a small code chunk.\n\nprint(f'Daniel has {2+2=} apples')\n\nDaniel has 2+2=4 apples"
  },
  {
    "objectID": "posts/welcome/post.html",
    "href": "posts/welcome/post.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Adventures with Langchain: Building Siri\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat’s going on in a HuggingFace Tokenizer?\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\nMy time in DC\n\n\n\n\n\nVisiting the Capitol\n\n\n\n\n\n\nDec 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJupyter Testing\n\n\n\n\n\nReviving Deep Dream\n\n\n\n\n\n\nNov 4, 2022\n\n\nDaniel\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGetting Quarto to execute code blocks\n\n\n\n\n\nReviving Deep Dream\n\n\n\n\n\n\nOct 5, 2022\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Daniel",
    "section": "",
    "text": "I’m a machine learning engineer.\nI shipped many safety critical features at Tesla Autopilot, working on FSD.\nI helped on the transition team at Twitter.\nI like to kite surf and tweet too much.\nI enjoy bike shedding over knowledge management systems."
  },
  {
    "objectID": "posts/my-time-in-dc/post.html",
    "href": "posts/my-time-in-dc/post.html",
    "title": "My time in DC",
    "section": "",
    "text": "I ran out of the huge glass doors of my hotel, expecting a face full of cold air. It was 4AM and pitch black outside, so I was shocked to feel warm humidity wash over my body as I left the hotel. Shrugging, I jogged on, following the street lights towards the Lincoln Memorial.\nIt was my second time in Washington DC, but my impression was the same: this is an amazing city full of history and cultural significance. This isn’t anything groundbreaking, but there’s a stark difference between the west coast and the east coast that I didn’t fully appreciate until I was in my late teenage years. As I watched the sun rise from the steps of the Lincoln Memorial, sweating in the humid air from my morning run, I had a distinct feeling of national pride that just doesn’t come from my morning run in San Francisco.\nIt sounds cliche, but the air genuinely felt different. I mean, it was humid as hell — so the air literally felt different, but beyond that: there was more ambition and stature in a way that SF doesn’t have. I love the tech entrepreneur vibes of Silicon Valley, but the architecture, statues, and monuments throughout DC make a clear statement: America does not mess around. I can’t imagine being a foreign diplomat or royalty on a visit to Washington, driving through this impressive city on a mission to negotiate with someone — I’d be intimidated before I even began the conversation!\nOn a more personal level, I was reminded of this same feeling the first time I wore a tailored suit, and the first time I wore a properly nice submariner watch. Even relatively simple outfit additions created meaningful behavior change for me. I carried myself differently, took myself more seriously, spoke more slowly. To this day, if I need a “confidence injection” for a big meeting or something, wearing a nice watch really has an impact.\nOkay — we can see how this plays out on a city-level: have a bunch of great architecture and monuments, and your citizens behave differently. And on a personal level: wear nice clothes, and you view yourself differently, which makes others view you differently.\nBut how do you add “stature improvements” to a small group? Or put another way… how do you build culture at your startup? Making everyone wear a nice watch or erecting a monument in the office probably isn’t the right move (this basically sounds like an episode of Billions). Put the mission statement on the wall, phrased in such a way that everyone is excited about it (”Our mission is to accelerate the world’s transition to sustainable energy”)? That’s a good start, but there’s more to the puzzle. Hiring the right people is obvious, but much easier said than done. There’s no magic bullet for customer building, but it’s useful to look at how nations and people do it — just another way that travel expands your horizons (literally) and gives you new ideas."
  },
  {
    "objectID": "posts/huggingface-tokenizer/post.html",
    "href": "posts/huggingface-tokenizer/post.html",
    "title": "Letters to my past self",
    "section": "",
    "text": "What’s going on in a HuggingFace Tokenizer?\nYou’ve probably seen this sort of sample code on the HF website, for example on the FLAN LLM page:\n\nprompt = \"A step by step recipe to make bolognese pasta:\"\n\n\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\", use_fast=False)\n\ninputs = tokenizer(\"A step by step recipe to make bolognese pasta:\", return_tensors=\"pt\")\noutputs = model.generate(**inputs)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n\n['Pour a cup of bolognese into a large bowl and add the pasta']\n\n\nThe LLM output is impressive, but I was curious what was going on under the hood of the tokenizer(...) line. What’s going on there, and how is the library designed to support all sorts of different encoding styles and parameters?\nI chose to investigate the use_fast=False option, although it looks like fast currently isn’t even an option for T5 (FLAN) tokenizers\nAs we’ll see, it mostly delegates to the SentencePiece library, but there’s a few layers of Python that reveals some nice helper functions, and showed me a bit more about how tokenization is handled by HuggingFace.\nWhen we call tokenizer(...), it’s really calling tokenizer.__call(...) under the hood – this is a typical Python syntactic sugar.\nUsing Jupyter’s nice ?? we can inspect the source code and start digging in:\nHere’s the call stack – most of these functions pretty trivially delegate to the next one in the list, with lots of error checking and batching and extra-argument passing.\ntokenizer.__call__ # delegates to _call_one...\ntokenizer._call_one # delegates to encode_plus...\ntokenizer.encode_plus # delegates to _encode_plus...\ntokenizer._encode_plus\n_encode_plus splits into three separate calls of interest (I am adding some additional variable names for legibility here):\ntokens = self.tokenize(text, **kwargs)\nids = self.convert_tokens_to_ids(tokens)\nself.prepare_for_model(ids, ...)\nLet’s look at them one by one:\n\n\ntokenize\ntokenizer.tokenize(prompt) # delegates to _tokenize...\ntokenizer._tokenize(prompt) # delegates to sp_model.encode...\ntokenizer.sp_model.encode(prompt, out_type=str)\nOkay, we’re down to the sp_model, aka the SentencePiece module that the tokenizer holds inside of it. This calls _EncodeAsPieces, which returns a list of strings – our first transformation! We’ve gone from a single prompt to a list of tokens. You can see how the more typical words (by, step, make) were not split up, but bolognese was. Maybe next time I’ll dig more into what words are part of the vocab and what aren’t, and why.\n\ntokenizer.sp_model._EncodeAsPieces(\n    text=prompt, \n    enable_sampling=tokenizer.sp_model._enable_sampling, \n    nbest_size=tokenizer.sp_model._nbest_size,\n    alpha=tokenizer.sp_model._alpha, \n    add_bos=tokenizer.sp_model._add_bos, \n    add_eos=tokenizer.sp_model._add_eos, \n    reverse=tokenizer.sp_model._reverse, \n    emit_unk_piece=tokenizer.sp_model._emit_unk_piece,\n)\n\n['▁A',\n '▁step',\n '▁by',\n '▁step',\n '▁recipe',\n '▁to',\n '▁make',\n '▁',\n 'b',\n 'ologne',\n 's',\n 'e',\n '▁pasta',\n ':']\n\n\nOk, _EncodeAsPieces is now at the _sentencepiece.so layer, no longer pure Python, which I’ll leave as an adventure for another day.\nThat was tokenization – let’s move on to…\n\n\nconvert_tokens_to_ids\ntokenizer.convert_tokens_to_ids\ntokenizer._convert_token_to_id_with_added_voc # basically a list comprehension, operates on each str, not the full list\ntokenizer._convert_token_to_id\ntokenizer.sp_model.piece_to_id\n\ntokenizer.sp_model.piece_to_id??\n\n\nSignature: tokenizer.sp_model.piece_to_id(arg)\nDocstring: <no docstring>\nSource:   \n  def _batched_func(self, arg):\n    if type(arg) is list:\n      return [_func(self, n) for n in arg]\n    else:\n      return _func(self, arg)\nFile:      ~/miniconda3/lib/python3.9/site-packages/sentencepiece/__init__.py\nType:      method\n\n\n\n\n\ntokenizer.sp_model.piece_to_id('▁A')\n\n71\n\n\n\ntokenizer.convert_tokens_to_ids(tokenizer.tokenize(prompt))\n\n[71, 1147, 57, 1147, 2696, 12, 143, 3, 115, 23443, 7, 15, 13732, 10]\n\n\nOkay, so now we’ve converted each human-readable token to ids! Notice step is 1147, occurring both at the 1st and 3rd index.\nOnce again it looks like we’re hitting the compiled layer (the source of sp_model.piece_to_id is wrong, IPython is getting confused), but it’s interesting how many Python layers you have to push through to get there.\nLet’s move on to the third and final call:\n\n\nprepare_for_model\n\ntokenizer.prepare_for_model(\n    ids=tokenizer.convert_tokens_to_ids(tokenizer.tokenize(prompt)),\n)\n\n{'input_ids': [71, 1147, 57, 1147, 2696, 12, 143, 3, 115, 23443, 7, 15, 13732, 10, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\nThis returns a dictionary with an attention mask, which isn’t that interesting in this specific example, and the input_ids, which is almost the same as the previous output of convert_tokens_to_ids. The only difference is a trailing 1, which is the eos_token, aka end-of-sequence token:\n\ntokenizer.convert_tokens_to_ids(tokenizer.eos_token)\n\n1\n\n\nSure enough, when I looked at the source for prepare_for_model, it does a bunch of different things (padding, truncation, backwards compatibility, etc) – but most importantly here, adds a EOS token.\nThe specific function that does it is build_inputs_with_special_tokens:\n\ntokenizer.build_inputs_with_special_tokens(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(prompt)))\n\n[71, 1147, 57, 1147, 2696, 12, 143, 3, 115, 23443, 7, 15, 13732, 10, 1]\n\n\n\ntokenizer.special_tokens_map['eos_token']\n\n'</s>'\n\n\n\n\nSummary\nSo – to wrap it all up – for this simple example copied from the HuggingFace website, you can get the same functionality from the tokenizer by breaking down to these smaller helper methods vs the higher level __call__\n\ntokenizer.prepare_for_model(\n    ids=tokenizer.convert_tokens_to_ids(tokenizer.tokenize(prompt)),\n)\n\n{'input_ids': [71, 1147, 57, 1147, 2696, 12, 143, 3, 115, 23443, 7, 15, 13732, 10, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\n\ntokenizer(prompt)\n\n{'input_ids': [71, 1147, 57, 1147, 2696, 12, 143, 3, 115, 23443, 7, 15, 13732, 10, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n\n\nThis was mostly an educational exercise to see what’s going on under the hood of the HuggingFace – I definitely wouldn’t recommend doing this sort of thing in production – but is a great way to get a bit deeper understanding of the library!"
  },
  {
    "objectID": "posts/langchain-adventures/post.html",
    "href": "posts/langchain-adventures/post.html",
    "title": "Letters to my past self",
    "section": "",
    "text": "There’s this fun library that I’ve seen on Twitter called LangChain. I wanted to take it for a spin and see if I could build some little fun thing with it – let’s see how it goes!\nLet’s start out with some boilerplate HuggingFace code, this is often useful when debugging:\n\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\ntokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n\ninputs = tokenizer(\"how tall is barack obama\", return_tensors=\"pt\")\noutputs = model.generate(**inputs)\nprint(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n\n['1.8 m']\n\n\nWhen chaining together LLMs, one obvious thing that comes to mind is modality-transition – or concretely, transitioning from audio -> text, and maybe even audio -> text -> image -> video (speak a movie into existence!)\nI know Whisper is popular for Audio -> Text, so let’s try to get that running:\n\nfrom pathlib import Path \nimport whisper\n\nmodel = whisper.load_model(\"base\")\nresult = model.transcribe(str(Path.home() / 'Downloads/testing.m4a'))\nprint(result[\"text\"])\n\n My name is Daniel. I live in California. I like to travel.\n\n\n\nCool – using the whisper library is quite easy! I tried to use the HuggingFace Whisper model, but ran into the following error:\nMemoryError: Cannot allocate write+execute memory for ffi.callback(). \nYou might be running on a system that prevents this. For more information, see \nhttps://cffi.readthedocs.io/en/latest/using.html#callbacks\nPossibly due to running locally on MacOS? I googled around for a bit and decided to just go with the Whisper library, as it worked out of the box, no need to fight with FFI issues.\nNow, let’s figure out how to record audio from a Jupyter Notebook. This is a bit clunky but not hard:\n\nfrom ipywebrtc import AudioRecorder, CameraStream\nimport torchaudio\nfrom IPython.display import Audio\n\ncamera = CameraStream(constraints={'audio': True, 'video': False})\nrecorder = AudioRecorder(stream=camera)\nrecorder\n\nNow, let’s bring in LangChain. They already have strong support for OpenAI and HuggingFace which is great, but I need a way to use the locally running Whisper model. Thankfully, I can add a new primitive and as long as I define the input_keys, output_keys, and a _call method, LangChain will happily accept it as part of a SequentialChain!\n\nfrom more_itertools import one\nfrom typing import List, Dict\n\nimport whisper\nfrom langchain.chains.base import Chain\n\nclass AudioToTextChain(Chain):\n    MODEL = whisper.load_model(\"base\")\n\n    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n        result = self.MODEL.transcribe(inputs[one(self.input_keys)], fp16=False)\n        return {one(self.output_keys): result['text']}\n\n    @property\n    def input_keys(self) -> List[str]:\n        return ['audio_fname']\n\n    @property\n    def output_keys(self) -> List[str]:\n        return ['transcription']\n\ntranscription_chain = AudioToTextChain()\n\nOkay, we have the Audio -> Text setup, now let’s use a small FLAN model to answer questions as a virtual assistant to create a dumber version of Siri!\nThis runs locally on my MacBook in a couple seconds, which isn’t too bad – but it doesn’t have any internet access. Maybe I’ll add that as a second step.\n\nfrom langchain.chains import SequentialChain\nfrom langchain import (\n    PromptTemplate, \n    HuggingFacePipeline, \n    LLMChain,\n)\n\ntemplate = \"\"\"You are a virtual assistant. Given a request, answer it to the best of your abilities.\n\nRequest:\n{transcription}\nAnswer:\"\"\"\n\nsiri_chain = LLMChain(\n    llm=HuggingFacePipeline.from_model_id('google/flan-t5-large', task='text2text-generation'), \n    prompt=PromptTemplate(input_variables=[\"transcription\"], template=template), \n    output_key=\"answer\",\n)\n\noverall_chain = SequentialChain(\n    chains=[transcription_chain, siri_chain],\n    input_variables=[\"audio_fname\"],\n    # Here we return multiple variables:\n    output_variables=[\"transcription\", \"answer\"],\n    verbose=True,\n)"
  }
]