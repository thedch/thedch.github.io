{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "507a2594-e385-45e3-9220-7dde151995a4",
   "metadata": {},
   "source": [
    "## Adventures with Langchain: Building Siri\n",
    "\n",
    "There's this fun library that I've seen on Twitter called [LangChain](https://github.com/hwchase17/langchain). I wanted to take it for a spin and see if I could build some little fun thing with it -- let's see how it goes!\n",
    "\n",
    "Let's start out with some boilerplate HuggingFace code, this is often useful when debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8eddb21-7e7c-4cbb-aa92-0d1cd88cf671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.8 m']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "inputs = tokenizer(\"how tall is barack obama\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59182b5-d993-4742-8df3-11c5c1e2c718",
   "metadata": {},
   "source": [
    "When chaining together LLMs, one obvious thing that comes to mind is modality-transition -- or concretely, transitioning from audio -> text, and maybe even audio -> text -> image -> video (speak a movie into existence!)\n",
    "\n",
    "I know Whisper is popular for Audio -> Text, so let's try to get that running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c393dade-f1aa-4d47-a2c0-589723aedc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " My name is Daniel. I live in California. I like to travel.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "result = model.transcribe(str(Path.home() / 'Downloads/testing.m4a'))\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b865392-3740-45dc-87b0-1cf75ac9b753",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "Cool -- using the whisper library is quite easy! I tried to use the [HuggingFace Whisper model](https://huggingface.co/docs/transformers/model_doc/whisper), but ran into the following error:\n",
    "\n",
    "```\n",
    "MemoryError: Cannot allocate write+execute memory for ffi.callback(). \n",
    "You might be running on a system that prevents this. For more information, see \n",
    "https://cffi.readthedocs.io/en/latest/using.html#callbacks\n",
    "```\n",
    "\n",
    "Possibly due to running locally on MacOS? I googled around for a bit and decided to just go with the Whisper library, as it worked out of the box, no need to fight with FFI issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598e0ad0-58c5-4769-acf5-a601c0891fa8",
   "metadata": {},
   "source": [
    "Now, let's figure out how to record audio from a Jupyter Notebook. This is a bit clunky but not hard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b493f443-3aa7-4fe0-beaa-2d5c15ddee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywebrtc import AudioRecorder, CameraStream\n",
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "\n",
    "camera = CameraStream(constraints={'audio': True, 'video': False})\n",
    "recorder = AudioRecorder(stream=camera)\n",
    "recorder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c67a90-2ba4-4393-af59-4193c03062ba",
   "metadata": {},
   "source": [
    "Now, let's bring in LangChain. They already have strong support for OpenAI and HuggingFace which is great, but I need a way to use the locally running Whisper model. Thankfully, I can add a new primitive and as long as I define the `input_keys`, `output_keys`, and a `_call` method, LangChain will happily accept it as part of a `SequentialChain`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60cdfe8-d39a-45e6-af25-f2d1cbf4ae98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from more_itertools import one\n",
    "from typing import List, Dict\n",
    "\n",
    "import whisper\n",
    "from langchain.chains.base import Chain\n",
    "\n",
    "class AudioToTextChain(Chain):\n",
    "    MODEL = whisper.load_model(\"base\")\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        result = self.MODEL.transcribe(inputs[one(self.input_keys)], fp16=False)\n",
    "        return {one(self.output_keys): result['text']}\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        return ['audio_fname']\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['transcription']\n",
    "\n",
    "transcription_chain = AudioToTextChain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdb6a5e-c88c-4645-a4a6-1aef9515ec72",
   "metadata": {},
   "source": [
    "Okay, we have the Audio -> Text setup, now let's use a small FLAN model to answer questions as a virtual assistant to create a dumber version of Siri!\n",
    "\n",
    "This runs locally on my MacBook in a couple seconds, which isn't too bad -- but it doesn't have any internet access. Maybe I'll add that as a second step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917f83e6-1740-45e5-9dbb-f319eee28a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "from langchain import (\n",
    "    PromptTemplate, \n",
    "    HuggingFacePipeline, \n",
    "    LLMChain,\n",
    ")\n",
    "\n",
    "template = \"\"\"You are a virtual assistant. Given a request, answer it to the best of your abilities.\n",
    "\n",
    "Request:\n",
    "{transcription}\n",
    "Answer:\"\"\"\n",
    "\n",
    "siri_chain = LLMChain(\n",
    "    llm=HuggingFacePipeline.from_model_id('google/flan-t5-large', task='text2text-generation'), \n",
    "    prompt=PromptTemplate(input_variables=[\"transcription\"], template=template), \n",
    "    output_key=\"answer\",\n",
    ")\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[transcription_chain, siri_chain],\n",
    "    input_variables=[\"audio_fname\"],\n",
    "    # Here we return multiple variables:\n",
    "    output_variables=[\"transcription\", \"answer\"],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aa4ee4-728f-494c-b386-8ea8d9de98d6",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "Okay, now let's run it! First, we record some audio, and then save it as `recording.webm`, which we feed in as the input to the `SequentialChain`, which is then transcribed by Whisper, and then processed by FLAN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba811f2-08ac-41c7-b782-2e36b162420a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9520bff8abb649178072faf486f35fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AudioRecorder(audio=Audio(value=b'', format='webm'), stream=CameraStream(constraints={'audio': True, 'video': â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywebrtc import AudioRecorder, CameraStream\n",
    "\n",
    "camera = CameraStream(constraints={'audio': True, 'video': False})\n",
    "recorder = AudioRecorder(stream=camera)\n",
    "recorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79274cc6-3e43-4187-b086-75a1599111a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\u001b[1mChain 0\u001b[0m:\n",
      "{'transcription': \" What's the best place to go surfing in Australia?\"}\n",
      "\n",
      "\u001b[1mChain 1\u001b[0m:\n",
      "{'answer': 'The Gold Coast'}\n",
      "\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "with open('recording.webm', 'wb') as f:\n",
    "    f.write(recorder.audio.value)\n",
    "\n",
    "review = overall_chain({\"audio_fname\": \"recording.webm\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fa634a-da8c-4e7b-884f-886c854511f5",
   "metadata": {},
   "source": [
    "There we go! Using the LangChain library and a few off the shelf models, we can create a little virtual assistant that runs locally and can answer all sorts of trivia questions!\n",
    "\n",
    "TODO:\n",
    "\n",
    "* Host it somewhere so I can ask it questions all day when I'm out\n",
    "* Train it on all my documents and data so it knows me\n",
    "* Connect it to all my applications / calendar / etc so it can take actions on my behalf when I ask it to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835622b-8666-4269-9826-ccf28955949c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
