{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "088f37a8-20ff-47e2-b03e-c3a5a08bccca",
   "metadata": {},
   "source": [
    "### What's going on in a HuggingFace Tokenizer?\n",
    "\n",
    "You've probably seen this sort of sample code on the HF website, for example on the [FLAN LLM page](https://huggingface.co/docs/transformers/model_doc/flan-t5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925299f2-3771-43b5-a0b9-206ff5239af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A step by step recipe to make bolognese pasta:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d405604e-3758-48c3-bed0-35e0e643a367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pour a cup of bolognese into a large bowl and add the pasta']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\", use_fast=False)\n",
    "\n",
    "inputs = tokenizer(\"A step by step recipe to make bolognese pasta:\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361abd1a-30d5-4008-bb3d-d689330dd766",
   "metadata": {},
   "source": [
    "The LLM output is impressive, but I was curious what was going on under the hood of the `tokenizer(...)` line.\n",
    "What's going on there, and how is the library designed to support all sorts of different encoding styles and parameters?\n",
    "\n",
    "I chose to investigate the `use_fast=False` option, although it looks like `fast` [currently isn't even an option for T5 (FLAN) tokenizers](https://huggingface.co/transformers/v2.9.1/main_classes/tokenizer.html)\n",
    "\n",
    "As we'll see, it mostly delegates to the SentencePiece library, but there's a few layers of Python that reveals some nice helper functions, and showed me a bit more about how tokenization is handled by HuggingFace.\n",
    "\n",
    "When we call `tokenizer(...)`, it's really calling `tokenizer.__call(...)` under the hood -- this is a typical Python syntactic sugar.\n",
    "\n",
    "Using Jupyter's nice `??` we can inspect the source code and start digging in:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed467a18-d2b5-4083-bb3e-91529e96dae9",
   "metadata": {},
   "source": [
    "Here's the call stack -- most of these functions pretty trivially delegate to the next one in the list,\n",
    "with lots of error checking and batching and extra-argument passing.\n",
    "\n",
    "```python\n",
    "tokenizer.__call__ # delegates to _call_one...\n",
    "tokenizer._call_one # delegates to encode_plus...\n",
    "tokenizer.encode_plus # delegates to _encode_plus...\n",
    "tokenizer._encode_plus\n",
    "```\n",
    "\n",
    "`_encode_plus` splits into three separate calls of interest (I am adding some additional variable names for legibility here):\n",
    "\n",
    "```python\n",
    "tokens = self.tokenize(text, **kwargs)\n",
    "ids = self.convert_tokens_to_ids(tokens)\n",
    "self.prepare_for_model(ids, ...)\n",
    "```\n",
    "\n",
    "Let's look at them one by one:\n",
    "\n",
    "### tokenize\n",
    "\n",
    "```\n",
    "tokenizer.tokenize(prompt) # delegates to _tokenize...\n",
    "tokenizer._tokenize(prompt) # delegates to sp_model.encode...\n",
    "tokenizer.sp_model.encode(prompt, out_type=str)\n",
    "```\n",
    "\n",
    "Okay, we're down to the `sp_model`, aka the SentencePiece module that the tokenizer holds inside of it. This calls `_EncodeAsPieces`, which returns a list of strings -- our first transformation! We've gone from a single prompt to a list of tokens. You can see how the more typical words (by, step, make) were not split up, but `bolognese` was. Maybe next time I'll dig more into what words are part of the vocab and what aren't, and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eb8a9d-82d6-42e5-917e-2bcd0865be1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁A',\n",
       " '▁step',\n",
       " '▁by',\n",
       " '▁step',\n",
       " '▁recipe',\n",
       " '▁to',\n",
       " '▁make',\n",
       " '▁',\n",
       " 'b',\n",
       " 'ologne',\n",
       " 's',\n",
       " 'e',\n",
       " '▁pasta',\n",
       " ':']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sp_model._EncodeAsPieces(\n",
    "    text=prompt, \n",
    "    enable_sampling=tokenizer.sp_model._enable_sampling, \n",
    "    nbest_size=tokenizer.sp_model._nbest_size,\n",
    "    alpha=tokenizer.sp_model._alpha, \n",
    "    add_bos=tokenizer.sp_model._add_bos, \n",
    "    add_eos=tokenizer.sp_model._add_eos, \n",
    "    reverse=tokenizer.sp_model._reverse, \n",
    "    emit_unk_piece=tokenizer.sp_model._emit_unk_piece,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095a7ab2-28e4-4902-8f59-198458c6c2cf",
   "metadata": {},
   "source": [
    "Ok, `_EncodeAsPieces` is now at the `_sentencepiece.so` layer, no longer pure Python, which I'll leave as an adventure for another day.\n",
    "\n",
    "That was tokenization -- let's move on to...\n",
    "\n",
    "### convert_tokens_to_ids\n",
    "\n",
    "```python\n",
    "tokenizer.convert_tokens_to_ids\n",
    "tokenizer._convert_token_to_id_with_added_voc # basically a list comprehension, operates on each str, not the full list\n",
    "tokenizer._convert_token_to_id\n",
    "tokenizer.sp_model.piece_to_id\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d020c9-3ad4-4b4c-bd5c-11aad2ef333e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpiece_to_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m <no docstring>\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "  \u001b[0;32mdef\u001b[0m \u001b[0m_batched_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m      \u001b[0;32mreturn\u001b[0m \u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/lib/python3.9/site-packages/sentencepiece/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.sp_model.piece_to_id??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b10c30-e613-4ec8-bdd4-0a11223c3c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sp_model.piece_to_id('▁A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f11e17-2fd5-4c1e-8354-d9a3626b63e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[71, 1147, 57, 1147, 2696, 12, 143, 3, 115, 23443, 7, 15, 13732, 10]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10253ee-628b-4092-834c-48dbb9c5c15d",
   "metadata": {},
   "source": [
    "Okay, so now we've converted each human-readable token to ids! Notice `step` is 1147, occurring both at the 1st and 3rd index.\n",
    "\n",
    "Once again it looks like we're hitting the compiled layer (the source of `sp_model.piece_to_id` is wrong, IPython is getting confused), but it's interesting how many Python layers you have to push through to get there.\n",
    "\n",
    "Let's move on to the third and final call:\n",
    "\n",
    "### prepare_for_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017b027c-9077-454b-8a10-32886b474a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [71, 1147, 57, 1147, 2696, 12, 143, 3, 115, 23443, 7, 15, 13732, 10, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.prepare_for_model(\n",
    "    ids=tokenizer.convert_tokens_to_ids(tokenizer.tokenize(prompt)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4be6ae5-3e7f-4af0-ac1e-70317f64c1b3",
   "metadata": {},
   "source": [
    "This returns a dictionary with an attention mask, which isn't that interesting in this specific example,\n",
    "and the `input_ids`, which is *almost* the same as the previous output of `convert_tokens_to_ids`. The only\n",
    "difference is a trailing `1`, which is the `eos_token`, aka end-of-sequence token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc58dca0-6d66-4a40-aebe-54df5aa52fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.eos_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77afa297-6819-49c4-abcf-e798d85751a7",
   "metadata": {},
   "source": [
    "Sure enough, when I looked at the source for `prepare_for_model`, it does a bunch of different things\n",
    "(padding, truncation, backwards compatibility, etc) -- but most importantly here, adds a EOS token.\n",
    "\n",
    "The specific function that does it is `build_inputs_with_special_tokens`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e19c95d-9dbf-4177-9540-3ccaa1ca9672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[71, 1147, 57, 1147, 2696, 12, 143, 3, 115, 23443, 7, 15, 13732, 10, 1]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.build_inputs_with_special_tokens(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(prompt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb30a73-d322-40fd-a524-e119a8a2af7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map['eos_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa55c58-c087-4562-83b4-ff7484c72917",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c57a44-3e64-49c9-b23c-bd7a183a9e19",
   "metadata": {},
   "source": [
    "So -- to wrap it all up -- for this simple example copied from the HuggingFace website, you can get the same functionality from the tokenizer by \n",
    "breaking down to these smaller helper methods vs the higher level `__call__`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12718fbe-14fe-4508-952c-145de6978478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [71, 1147, 57, 1147, 2696, 12, 143, 3, 115, 23443, 7, 15, 13732, 10, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.prepare_for_model(\n",
    "    ids=tokenizer.convert_tokens_to_ids(tokenizer.tokenize(prompt)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6226d41-8956-4d34-8fa1-329304b7626c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [71, 1147, 57, 1147, 2696, 12, 143, 3, 115, 23443, 7, 15, 13732, 10, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f09343-6cde-4580-a896-ef95ead17663",
   "metadata": {},
   "source": [
    "This was mostly an educational exercise to see what's going on under the hood of the HuggingFace -- I definitely\n",
    "wouldn't recommend doing this sort of thing in production -- but is a great way to get a bit deeper understanding of the library!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
